---
title: "(Introduction au) TAL/NLP avec Python"
subtitle: "De la lexicom√©trie aux mod√®les pr√©-entrain√©s"
author: "√âmilien Schultz & L√©o Mignot"
format:
#    pptx
    revealjs:
        slideNumber: true
---


# Vous avez dit NLP üî§ ?

## Traitement des donn√©es textuelles au sens large

Une p√©riode troubl√©e : explosion des usages

- des approches tr√®s vari√©es
- venant d'√©poques diff√©rentes
- avec des outils diff√©rents

## Le texte : donn√©es non structur√©es

- *Au d√©but* : une suite de caract√®res 
    - Avec un encodage : binary ‚û°Ô∏è symbole
- *√Ä la fin* : du sens intelligible
    - Proches de l'humain

*Entre* : comment structurer l'information ?

---

## Un domaine interdisciplinaire


![](img/nlpfield.jpg){fig-align="center"}

::: {style="font-size: 50%"}

<center>
[Bibliometric analysis of natural language processing using CiteSpace and VOSviewer](https://www.sciencedirect.com/science/article/pii/S2949719124000712)
</center>

:::

---

## Des techniques diff√©rentes

- **Symbolique** ancrage linguistique
    - se baser sur les r√®gles / th√©orie du language
- **Statistiques** (compter des mots)
    - un peu *old school*
- **Machine Learning** sur donn√©es textuelles vectoris√©es
- √âvolution vers les **repr√©sentations**
    - *mod√®les de langage*s
    - Approches potentiellement plus proche du langage naturel

*Chacune a ses particularit√©s/limites/co√ªts/outils*

---

## Quoi faire avec du texte ?

Des traitements tr√®s vari√©s

- Classifier des textes
    - Faire des groupes
    - Retrouver des √©l√©ments
- Identifier des √©l√©ments sp√©cifiques
    - Noms propres, etc.
- G√©n√©rer des textes ...

---

## De tr√®s nombreuses t√¢ches

![](./img/nlp_tasks.png){fig-align="center"}


---

## Donc 

- Pour une m√™me t√¢che, des techniques tr√®s vari√©es
    - par exemple identifier les th√©matiques
- Pour une techique, des applications tr√®s diff√©rentes
    - retrouver des √©l√©ments vs. les visualiser
- Des m√©triques d'√©valuation diff√©rentes
    - F1, corr√©lation, etc.
- Des mises en oeuvre multiples
    - Logiciels, programmation, etc.

*Difficile de tout faire :)*


# Quelques notions

## Point de d√©part : du texte

Texte ‚û°Ô∏è Repr√©sentation num√©rique

C'est quoi un texte ? Diversit√© de supports

- Document avec des chaines de caract√®res num√©riques
- Pas encore mis en forme (PDF, images)
    - Enjeux d'OCR, de spatialisation (frame)

---

## Constituer un corpus exploitable

Une √©tape √† part enti√®re (qui peut prendre plus de temps que tout le reste)

- techniques de traitement d'image (segmentation)
- image to text (OCR)
- manipulation de donn√©es

---

## Num√©riques mais non structur√©

- Diff√©rentes langues (m√©lang√©es)
- Des erreurs (OCR)
- Des √©l√©ments suppl√©mentaires non textuels (√©moticones)
- Et plus ...


Et tous les probl√®mes li√©s aux repr√©sentations num√©riques des textes (formats, encodage)

---

## Structurer implique de faire des choix

Quelle unit√© de base choisir ?

- Document entier
- Paragraphe
- Phrase
- Mot / couple de mot (bigramme)
- ...

(D√©pendance √† la langue √©videmment)


---

## Repr√©sentation d'un texte

Il n'y a pas une seule fa√ßon de penser un texte :

- Une suite de lettres
- Une suite de mots
- Des mots li√©s les uns aux autres
- Des √©l√©ments pertinents (noms propres, mots cl√©, etc.) li√©s aux autres


---

## Diff√©rentes m√©thodes de repr√©sentation

- Par la pr√©sence de certains mots
    - Approches par dictionnaires
    - Ou par motifs : expressions r√©guli√®res
- Par l'ensemble des mots
    - Approches par *sacs de mots* (*bags of words*)
- Par encodage de la structure
    - Approches par plongement (embeddings) contextuels ou non

---

## Historiquement, l'importance des mots

Importance de l'unit√© de base du mot

- d√©coupage en mots
- suppression des mots vides
- lemmatisation/stemmisation (racinisation, stemmatisation)

Pour de nombreux besoins sp√©cifiques, int√©ressant de ma√Ætriser les manipulations de bas niveau

---

## Tok√©nizer : une op√©ration complexe

![](img/tokenization.svg){fig-align="center"}

---

## Faire des statistiques - la lexicom√©trie

Analyse *fr√©quentiste*

- Comptage
- Sp√©cificit√© sur certains documents
- Indicateurs sp√©cifiques (mots compliqu√©s, hapax, etc.)
- Evolution / croisement avec d'autres variables
- Mod√©lisation des distributions
    - LDA que je ne pr√©senterai pas :)

---

## Du token √† la repr√©sentation

Passer d'un texte √† un vecteur num√©rique sur l'ensemble de l'unit√© textuelle repr√©sent√©e.

![](img/representation.png){fig-align="center"}

---

## Du token mot au token segment

Avec le ML, d'autres unit√©s sont importantes

- Passage en unit√©s discr√®tes
    - **Tokenisation par mots** : "Je vais bien" ‚Üí `["Je", "vais", "bien"]`
    - **Sous-mots (Byte-Pair Encoding, WordPiece)** : 
    - "inconnue" ‚Üí `["in", "##con", "##nue"]`
    - **Caract√®res** : chaque caract√®re est un token ‚Üí `["J", "e", " ", "v", ...]`
- D√©pend du pipeline/cons√©quences importantes

---

## Que faire avec une repr√©sentation

- Facilit√© de comparer deux vecteurs
- Calculer des distances
- Utiliser des mod√®les "classiques" de machine learning (ML)
- Faire des repr√©sentations (d√©compositions factorielles, etc)

![](img/cossimilarity.png){fig-align="center"}

---

## Parenth√®se : notions de machine learning

Notions plus g√©n√©rales que le NLP

- Apprentissage non-supervis√©
    - Utiliser la structure propre d'un jeu de donn√©es (ex. cluster, repr√©sentations)
- Apprentissage supervis√©
    - Utiliser de l'information donn√©e par l'utilisateur pour guider


---

## Deep learning et mod√®les

- Augmentation des corpus & des tailles de mod√®les
- Possibilit√© d'entra√Æner des mod√®les √† 
    - repr√©senter
    - pr√©dire
- De plus en plus de mod√®les pr√©entrain√©s

---

## Arriv√©e des embeddings

Espaces latents construits par entrainement de mod√®les sur des grands corpus (pr√©diction) : repr√©sentations denses

![](./img/word_embedding.png){fig-align="center"}

D'abord non contextuels puis contextuels


---

## De plus en plus de mod√®les


- Mod√®les de langage
    - Encoder les r√©gularit√©s de corpus
    - Diff√©rentes architectures & tailles
- Mod√®les fondationnels
    - Utilisable pour des t√¢ches sp√©cifiques
    - Propri√©t√©s √©mergentes


---


## Une multitude de mod√®les

![](img/model_history.png){width=500}

https://blog.dataiku.com/nlp-metamorphosis



---

## Quelques remarques en passant

- Importance du corpus d'entrainement
    - Sp√©cifiques √† la langue / type de textes
- D√©pendent de plusieurs niveaux
    - Tokenisation
    - Corpus d'entrainement
    - M√©thodes (RLHF)...
- De nouvelles notions : 
    - fen√™tre de contexte
    - BERT, GPT, ...


---

## En ce qui nous concerne

- Transformers
    - BERT (encoder only, 2018+)
        - pour le fran√ßais [CamemBert](https://camembert-model.fr/) ou [FlauBERT](https://huggingface.co/docs/transformers/en/model_doc/flaubert)
        - R√©cemment, [ModernBERT](https://huggingface.co/blog/modernbert)
- Depuis 2022, explosion des LLM
    - D√©passe le NLP (par exemple, Whisper)
    - [HuggingFace](https://huggingface.co/)


---

## Mod√®les BERT

![](./img/bert_article.png){fig-align="center"}

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
](https://arxiv.org/abs/1810.04805)

---

## Entrainement, t√¢ches et fine-tuning

- Les grands mod√®les fondationnels sont lourds √† entra√Æner (jours avec beaucoup de ressources)
- Une pratique de fine-tuning (apprentissage supervis√© ou semi-supervis√©)
- Mod√®le pr√©-entrain√© > utilisation sur diff√©rentes t√¢ches

---

## Des mod√®les de toutes tailles

[Un petit tour sur Hugging Face](https://huggingface.co/)

- Indicateur : nombre de param√®tres
- Des mod√®les de grande taille
    - Certains n√©cessitent des GPU
    - Penser √† utiliser des services d√©di√©s si n√©cessaire

**Un enjeu : manipuler ces mod√®les**

---


## √âvaluer une t√¢che

- Suivant la t√¢che (classification, d√©tection de mots, etc.) l'√©valuation va √™tre diff√©rente.
- Approche machine learning qui apprend sur un corpus :
    - contrainte de g√©n√©raliser au-del√† du corpus vu (√©viter le sur-apprentissage)
    - train/eval/test pour d√©terminer le meilleur mod√®le et avoir sa performance r√©elle

üìå Pas une seule m√©trique ‚Äúmagique‚Äù : le choix d√©pend du contexte !

---

## Cas fr√©quent : classification

Matrice de confusion : Vrais positifs (VP), faux positifs (FP), faux n√©gatifs (FN), vrais n√©gatifs (VN)

- Exactitude (Accuracy) : Proportion de bonnes pr√©dictions
- Pr√©cision (Precision) : Parmi les pr√©dits comme ‚Äúpositifs‚Äù, combien sont corrects ?
- Rappel (Recall) : Parmi les vrais ‚Äúpositifs‚Äù, combien sont retrouv√©s ?
- F1-score :Moyenne harmonique de pr√©cision et rappel

---

## Un mot sur les biais

- Les mod√®les pr√©-entrain√©s le sont sur des corpus
- De compositions variables
- Avec diff√©rentes formes de post-entrainement

Par exemple, biais genr√©s ; manque de certaines langues ; etc.

---

## Un mot sur l'ouverture 

Diff√©rents degr√©s d'ouverture 

- Compl√®tement ferm√©
- Poids ouverts
- M√©thodes d'entrainement connues
- Donn√©es disponibles

---

## Enjeu en g√©n√©ral

Suivant les besoins, trouver la bonne t√¢che :

- rapidit√©
- robustesse
- efficacit√©
- ...

Et importance d'√©valuer la qualit√© du traitement.

**Faisons un petit tour des lieux**


# Faire du NLP avec Python

## Python dans tout √ßa

- Language de programmation permettant la manipulation des donn√©es
- Au coeur de la r√©volution IA actuelle

---

## Les biblioth√®ques Python

- Avant, un peu p√©rim√©e `NLTK`
- Pour faire du ML avec `Scikit-learn`
- Le plus pratique : `SpaCy`
- Des mod√®les d√©di√©s : `GenSim`
- Utiliser directement des mod√®les de HuggingFace avec `Transformers`
    - Ou des biblioth√®ques construites dessus ...

---

## M√©lange savoir sp√©cialis√©s / comp√©tences g√©n√©riques

- Manipuler des donn√©es
    - petites/larges
- Notions de ML
- Biblioth√®ques sp√©cialis√©es



# Passons √† la pratique

---

## Pour aller plus loin

Beaucoup de litt√©rature & de tutoriaux

- [le cours de Lino Galliana](https://pythonds.linogaliana.fr/content/NLP/)
- [Speech and Language Processing (3rd ed. draft), Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/)
- [Text As Data](https://press.princeton.edu/books/hardcover/9780691207544/text-as-data?srsltid=AfmBOop3v1immdH8iAY34EQgGpfPPrRPOxBlg93s6Ch-giikMeWa3dw7)
