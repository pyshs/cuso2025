{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structurer et explorer des données textuelles\n",
    "\n",
    "Notebook Introduction au traitement du langage naturel - 15/05/2025 - Émilien Schultz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données\n",
    "\n",
    "Open Alex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les bibliothèques\n",
    "\n",
    "- `pandas` pour la manipulation de données\n",
    "- `nltk` pour le traitement de texte\n",
    "- `matplotlib` pour la visualisation\n",
    "- `scikit-learn` pour le traitement de texte et la modélisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas nltk scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer les données (préprocessing)\n",
    "\n",
    "- Supprimer les doublons\n",
    "- Supprimer les lignes vides\n",
    "- Convertir en minuscules\n",
    "- Garder uniquement de l'anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/CSS_exact_openalex.csv\")\n",
    "\n",
    "df[\"abstract\"].isna().sum()\n",
    "df = df[~df[\"abstract\"].isna()]\n",
    "\n",
    "df[\"texte\"] = df[\"title\"] + \" \" + df[\"abstract\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse à l'échelle des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher la présence d'un mot\n",
    "\n",
    "Les bases de la fouille de données. Quels sont les questions qui parlent d'intelligence artificielle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtre = df[\"texte\"].str.contains(\"algorithm\")\n",
    "filtre.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chercher un contexte d'un mot avec uen expression régulière"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['work algorithms, wh',\n",
       " 'bust algorithms for',\n",
       " 'n an algorithm is p',\n",
       " \" The algorithm's go\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(\".{5}algorithm.{5}\", df[filtre].loc[304, \"texte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[\"texte\"].str.lower()\n",
    "            .str.contains(\"artificial intelligence\")\n",
    "            .sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et si on cherche plusieurs termes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termes = [\"AI\", \"algorithm\"]\n",
    "\n",
    "(df[\"texte\"].str.lower()\n",
    "            .str.contains(\"|\".join(termes))\n",
    "            .sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire une recherche sur toutes les variables possibles de l'IA\n",
    "\n",
    "- intelligence artificelle\n",
    "- algorithme\n",
    "- AI\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "Découper un texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser les regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ceci', 'est', 'un', 'test']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "word_pattern = r\"\\w+\"\n",
    "tokens = re.findall(word_pattern, \"Ceci est un test\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [computational, social, science, 14, 0642, 033...\n",
       "1       [manifesto, of, computational, social, science...\n",
       "3       [computational, social, science, and, sociolog...\n",
       "7       [can, large, language, models, transform, comp...\n",
       "9       [on, agent, based, modeling, and, computationa...\n",
       "                              ...                        \n",
       "1436    [area, studies, and, the, challenges, of, crea...\n",
       "1439    [we, talk, data, we, do, data, welcome, to, th...\n",
       "1441    [prediction, machines, my, essay, has, several...\n",
       "1447    [index, citation, 2020, index, härtel, c, e, j...\n",
       "1448    [index, citation, 2023, index, lytras, m, d, h...\n",
       "Name: texte, Length: 690, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"texte\"].apply(lambda x: re.findall(r\"\\w+\",x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser une première bibliothèque : `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/leo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ceci', 'est', 'un', 'test']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Ceci est un test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Computational, Social, Science, 14,0642,033Me...\n",
       "1       [Manifesto, of, computational, social, science...\n",
       "3       [Computational, Social, Science, and, Sociolog...\n",
       "7       [Can, Large, Language, Models, Transform, Comp...\n",
       "9       [On, agent-based, modeling, and, computational...\n",
       "                              ...                        \n",
       "1436    [Area, Studies, and, the, Challenges, of, Crea...\n",
       "1439    [We, talk, data, ., We, do, data, ., Welcome, ...\n",
       "1441    [Prediction, Machines, My, essay, has, several...\n",
       "1447    [Index, Citation, (, 2020, ), ,, ``, Index, ''...\n",
       "1448    [Index, Citation, (, 2023, ), ,, ``, Index, ''...\n",
       "Name: texte, Length: 690, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"texte\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quels sont les termes les plus fréquents ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compteur = Counter([j for i in list(df[\"texte\"].apply(word_tokenize)) for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 9699),\n",
       " ('the', 6493),\n",
       " ('of', 6025),\n",
       " ('and', 5878),\n",
       " ('.', 5125),\n",
       " ('to', 3420),\n",
       " ('in', 3088),\n",
       " ('a', 2415),\n",
       " ('social', 1953),\n",
       " ('for', 1601),\n",
       " (')', 1491),\n",
       " ('(', 1471),\n",
       " ('on', 1340),\n",
       " ('that', 1282),\n",
       " ('data', 1180),\n",
       " ('is', 1061),\n",
       " ('as', 942),\n",
       " ('science', 912),\n",
       " ('with', 901),\n",
       " ('computational', 887)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compteur.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelles sont les expressions qui reviennent le plus souvent ?\n",
    "\n",
    "Utilisons les bigrammes et les trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_bigrams_nltk(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    return bigrams\n",
    "\n",
    "#generate_bigrams_nltk(df[\"texte_net\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ici-même', ',', 'y', \"'\", \"a-t'il\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"ici-même, y'a-t'il\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enlever les stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emilien/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = list(set(stopwords.words(\"english\")))\n",
    "english_stopwords[0:10]\n",
    "\n",
    "\n",
    "def generate_bigrams_nltk(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in english_stopwords]\n",
    "    bigrams = list(ngrams(filtered_tokens, 2))\n",
    "    return bigrams\n",
    "\n",
    "#generate_bigrams_nltk(df[\"texte_net\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "social science          882\n",
       "computational social    872\n",
       "social media            350\n",
       "social sciences         189\n",
       "big data                169\n",
       "                       ... \n",
       "socioeconomic status      9\n",
       "comparative analysis      9\n",
       "strength social           9\n",
       "age gender                9\n",
       "models social             9\n",
       "Length: 300, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count bigrams:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=french_stopwords, ngram_range=(2, 2), max_features=300)\n",
    "bigrammes = (\n",
    "    pd.DataFrame(\n",
    "        vectorizer.fit_transform(df[\"texte\"]).toarray(),\n",
    "        columns=vectorizer.get_feature_names_out(),\n",
    "    )\n",
    "    .T.sum(axis=1)\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "bigrammes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représenter les textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Présence de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f9/d2d_05ws5gncmml0fx0c00kw0000gp/T/ipykernel_77900/386222696.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[[\"dim1\",\"dim2\"]].replace({True:1,False:0}).head()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dim1  dim2\n",
       "0     0     0\n",
       "1     0     1\n",
       "3     0     1\n",
       "7     0     1\n",
       "9     0     1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"dim1\"] = df[\"texte\"].str.contains(\"AI\")\n",
    "df[\"dim2\"] = df[\"texte\"].str.contains(\"science\")\n",
    "df[[\"dim1\",\"dim2\"]].replace({True:1,False:0}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vecteur brut : Document term matrix / tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# créer mon object de ML\n",
    "vectorizer = CountVectorizer(stop_words=english_stopwords, \n",
    "                             ngram_range=(1, 1), \n",
    "                             max_features=800)\n",
    "\n",
    "# appliquer sur les données\n",
    "X = vectorizer.fit_transform(df[\"texte\"])\n",
    "X = pd.DataFrame(X.toarray(),columns=list(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Une version un peu plus avancée\n",
    "\n",
    "- Term Frequency-Inverse Document Frequency\n",
    "    - Amélioration du DTM\n",
    "- Approche souvent utilisée pour mettre en valeur les mots les plus spécifiques\n",
    "- `Scikit-learn` a `TfidfVectorizer`\n",
    "\n",
    "$$\\text{TF-IDF}(t, d, D) = \\left( \\frac{f_{t,d}}{n_d} \\right) \\times \\log \\left(\\frac{N}{\\text{df}_t} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10               0.000000\n",
       "politics         0.000000\n",
       "popular          0.000000\n",
       "popularity       0.000000\n",
       "population       0.000000\n",
       "                   ...   \n",
       "even             0.000000\n",
       "era              0.000000\n",
       "social           0.559948\n",
       "computational    0.560759\n",
       "science          0.609924\n",
       "Name: 100, Length: 800, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# créer un objet\n",
    "vectorizer = TfidfVectorizer(stop_words=english_stopwords, \n",
    "                             ngram_range=(1, 1), \n",
    "                             max_features=800)\n",
    "\n",
    "# applique \n",
    "X = vectorizer.fit_transform(df[\"texte\"])\n",
    "\n",
    "# mettre en forme\n",
    "X = pd.DataFrame(X.toarray(),columns=list(vectorizer.get_feature_names_out()))\n",
    "X.loc[100].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire la matrice TF-IDF, identifier les mots qui ont le score le plus important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance entre deux textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02197339]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"texte\"])\n",
    "cosine_similarity(X[0], X[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pd.DataFrame(pairwise_distances(X, metric=\"cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances[10].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application : Faire un nuage de mots avec WordCloud\n",
    "\n",
    "Un coup d'oeil à la [documentation](https://amueller.github.io/word_cloud/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
